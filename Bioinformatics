
#################################
#####pre-variant filtering ######
#################################

##filter optical duplicates with bbtools clumpify

for f in `ls -1 *_001.fastq.gz | sed 's/_R[1,2]_001.fastq.gz//' `
do
clumpify.sh in1=${f}_R1_001.fastq.gz in2=${f}_R2_001.fastq.gz out1=${f}_R1.uni.fastq.gz out2=${f}_R2.uni.fastq.gz dedupe optical dupedist=1200
done


##filter for quality and remove adapters with TRIMMOMATIC. nextadapt.fa is a file containing Illumina adapter sequences
for f in `ls -1 *.uni.fastq.gz | sed 's/_R[1,2].uni.fastq.gz//' `
do
trimmomatic PE -threads 4 -phred33 ${f}_R1.uni.fastq.gz ${f}_R2.uni.fastq.gz \
${f}.R1.uni.trim.fastq.gz ${f}.R1.uni.untrim.fastq.gz \
${f}.R2.uni.trim.fastq.gz ${f}.R2.uni.untrim.fastq.gz \
ILLUMINACLIP:nextadapt.fa:2:30:10:8:TRUE LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:30
done


##filter contaminant reads with fastqscreen. fastq_screen.conf.alt contains paths to indexed genomes from potential contaminants from genbank. command imbedded within python script allowing restart from last completed sample

OMP_NUM_THREADS=12
srun python fastq_script.py

import os
from os.path import exists
import csv
import time

startVar = 0

if exists('restart.in'):
     with open('restart.in','r') as ioText:
         startVar = int(ioText.readlines()[0].strip())+1
     print("Start value is ",str(startVar))
     time.sleep(5)

file_array = []
with open('filelist.txt', 'r') as iowrapper:
    csvreader = csv.reader(iowrapper, delimiter=',')
    for row in csvreader:
        file_array.append(row[0])

for i in range(startVar,100):
    filebase = file_array[i]
    os.system(f"fastq_screen --threads 12 --subset 0 --conf /miniconda3/share/fastq-screen-0.14.0-0/fastq_screen.conf.alt --tag --filter 00000000000000000000 --force --aligner B$
    print(i)
    # write i into restart file
    with open('restart.in','w') as iowrapper:
        iowrapper.write(str(i))


##remove pairs for which the mate was filtered out with bbtools
for f in `ls -1 *.uni.trim.tagged_filter.fastq.gz | sed 's/.R[1,2].uni.trim.tagged_filter.fastq.gz//' `
do
repair.sh in=${f}.R1.uni.trim.tagged_filter.fastq.gz in2=${f}.R2.uni.trim.tagged_filter.fastq.gz out=${f}.R1.uni.repair.tagged_filter.fastq.gz out2=${f}.R2.uni.repair.tagged_filter.fastq.gz
done


##filter polyg tails with cutadapt
for f in `ls -1 *.uni.repair.tagged_filter.fastq.gz | sed 's/.R[1,2].uni.repair.tagged_filter.fastq.gz//' `
do
cutadapt -j 12 -a "G{10}" -A "G{10}" \
         -o ${f}.R1.uni.repair.filter.g.fastq.gz -p ${f}.R2.uni.repair.filter.g.fastq.gz \
         ${f}.R1.uni.repair.tagged_filter.fastq.gz ${f}.R2.uni.repair.tagged_filter.fastq.gz
done


##kmer filter low variation reads with bbtools
for f in `ls -1 *.uni.repair.filter.g.fastq.gz | sed 's/.R[1,2].uni.repair.filter.g.fastq.gz//' `
do
bbduk.sh in1=${f}.R1.uni.repair.filter.g.fastq.gz in2=${f}.R2.uni.repair.filter.g.fastq.gz \
         out1=${f}.R1.uni.repair.filter.g.kmer.fastq.gz out2=${f}.R2.uni.repair.filter.g.kmer.fastq.gz \
         entropy=0.5 entropywindow=50 entropyk=5 \
         stats=${f}.bbduk_entropy_stats.txt \
         threads=12
done

##align reads to Myo_luc2.0 genome using bwa. command imbedded within python script to allow restart from last completed sample after interuption. mylu.rm.bwa is the indexed genome
OMP_NUM_THREADS=20
srun python bwa_script.new.py

import os
from os.path import exists
import csv
import time

startVar = 0

if exists('restart.bwa.x.in'):
     with open('restart.bwa.x.in','r') as ioText:
         startVar = int(ioText.readlines()[0].strip())+1
     print("Start value is ",str(startVar))
     time.sleep(5)

file_array = []
with open('filelist.txt', 'r') as iowrapper:
    csvreader = csv.reader(iowrapper, delimiter=',')
    for row in csvreader:
        file_array.append(row[0])

for i in range(startVar,70):
    filebase = file_array[i]
    os.system(f"bwa mem -t 20 -T 40 -L 500 -B 16 -M mylu.rm.bwa {filebase}.R1.uni.repair.filter.g.kmer.fastq.gz {filebase}.R2.uni.repair.filter.g.kmer.fastq.gz > $
    print(i)
    # write i into restart file
    with open('restart.bwa.x.in','w') as iowrapper:
        iowrapper.write(str(i))


##convert sam files to bam files with samtools
for f in `ls -1 *.sam | sed 's/.sam//' `
do
samtools view -@ 20 -bS ${f}.sam > ${f}.bam
done


##sort bam files with samtools
for f in `ls -1 *.bam | sed 's/.bam//' `
do
samtools sort ${f}.bam -@ 20 -o ${f}.sorted.bam
done


##remove improperly paired reads with samtools
for f in `ls -1 *.sorted.bam | sed 's/.sorted.bam//' `
do
samtools view -b -f 0x2 ${f}.sorted.bam -@ 20 > ${f}.sorted.prop.bam
done


##analyze data with samtools flagstat
for f in `ls -1 *.prop.bam | sed 's/.prop.bam//' `
do
samtools flagstat ${f}.prop.bam -@ 20 > outputfiles/${f}.prop.out.flag
done


##filter duplicates with picard
for f in `ls -1 *.sorted.prop.bam | sed 's/.sorted.prop.bam//' `
do
picard -Xmx100g MarkDuplicates \
      REMOVE_DUPLICATES=true \
      I=${f}.sorted.prop.bam \
      O=${f}.sorted.prop.dups.bam \
      M=${f}.marked_dup_metrics.txt
done


##calculate coverage metrics with picard
for f in `ls -1 *.dups.bam | sed 's/.dups.bam//' `
do
picard -Xmx100g CollectWgsMetrics \
      I=${f}.dups.bam \
      O=${f}.coverage.txt \
      R=/scratch/rek89/genome/mylu.genome.rm.gmap.k100.fa
done


##index with samtools
for f in `ls -1 *.sorted.prop.dups.bam | sed 's/.sorted.prop.dups.bam//' `
do
samtools index ${f}.sorted.prop.dups.bam -@ 20
done
