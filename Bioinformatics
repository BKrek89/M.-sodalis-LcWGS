
#################################
#####pre-variant filtering ######
#################################

##filter optical duplicates with bbtools clumpify

for f in `ls -1 *_001.fastq.gz | sed 's/_R[1,2]_001.fastq.gz//' `
do
clumpify.sh in1=${f}_R1_001.fastq.gz in2=${f}_R2_001.fastq.gz out1=${f}_R1.uni.fastq.gz out2=${f}_R2.uni.fastq.gz dedupe optical dupedist=1200
done


##filter for quality and remove adapters with TRIMMOMATIC. nextadapt.fa is a file containing Illumina adapter sequences
for f in `ls -1 *.uni.fastq.gz | sed 's/_R[1,2].uni.fastq.gz//' `
do
trimmomatic PE -threads 4 -phred33 ${f}_R1.uni.fastq.gz ${f}_R2.uni.fastq.gz \
${f}.R1.uni.trim.fastq.gz ${f}.R1.uni.untrim.fastq.gz \
${f}.R2.uni.trim.fastq.gz ${f}.R2.uni.untrim.fastq.gz \
ILLUMINACLIP:nextadapt.fa:2:30:10:8:TRUE LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:30
done


##filter contaminant reads with fastqscreen. fastq_screen.conf.alt contains paths to indexed genomes from potential contaminants from genbank. command imbedded within python script allowing restart from last completed sample

OMP_NUM_THREADS=12
srun python fastq_script.py

import os
from os.path import exists
import csv
import time

startVar = 0

if exists('restart.in'):
     with open('restart.in','r') as ioText:
         startVar = int(ioText.readlines()[0].strip())+1
     print("Start value is ",str(startVar))
     time.sleep(5)

file_array = []
with open('filelist.txt', 'r') as iowrapper:
    csvreader = csv.reader(iowrapper, delimiter=',')
    for row in csvreader:
        file_array.append(row[0])

for i in range(startVar,100):
    filebase = file_array[i]
    os.system(f"fastq_screen --threads 12 --subset 0 --conf /miniconda3/share/fastq-screen-0.14.0-0/fastq_screen.conf.alt --tag --filter 00000000000000000000 --force --aligner B$
    print(i)
    # write i into restart file
    with open('restart.in','w') as iowrapper:
        iowrapper.write(str(i))


##remove pairs for which the mate was filtered out with bbtools
for f in `ls -1 *.uni.trim.tagged_filter.fastq.gz | sed 's/.R[1,2].uni.trim.tagged_filter.fastq.gz//' `
do
repair.sh in=${f}.R1.uni.trim.tagged_filter.fastq.gz in2=${f}.R2.uni.trim.tagged_filter.fastq.gz out=${f}.R1.uni.repair.tagged_filter.fastq.gz out2=${f}.R2.uni.repair.tagged_filter.fastq.gz
done


##filter polyg tails with cutadapt
for f in `ls -1 *.uni.repair.tagged_filter.fastq.gz | sed 's/.R[1,2].uni.repair.tagged_filter.fastq.gz//' `
do
cutadapt -j 12 -a "G{10}" -A "G{10}" \
         -o ${f}.R1.uni.repair.filter.g.fastq.gz -p ${f}.R2.uni.repair.filter.g.fastq.gz \
         ${f}.R1.uni.repair.tagged_filter.fastq.gz ${f}.R2.uni.repair.tagged_filter.fastq.gz
done


##kmer filter low variation reads with bbtools
for f in `ls -1 *.uni.repair.filter.g.fastq.gz | sed 's/.R[1,2].uni.repair.filter.g.fastq.gz//' `
do
bbduk.sh in1=${f}.R1.uni.repair.filter.g.fastq.gz in2=${f}.R2.uni.repair.filter.g.fastq.gz \
         out1=${f}.R1.uni.repair.filter.g.kmer.fastq.gz out2=${f}.R2.uni.repair.filter.g.kmer.fastq.gz \
         entropy=0.5 entropywindow=50 entropyk=5 \
         stats=${f}.bbduk_entropy_stats.txt \
         threads=12
done

##align reads to Myo_luc2.0 genome using bwa. command imbedded within python script to allow restart from last completed sample after interuption. mylu.rm.bwa is the indexed genome
OMP_NUM_THREADS=20
srun python bwa_script.new.py

import os
from os.path import exists
import csv
import time

startVar = 0

if exists('restart.bwa.x.in'):
     with open('restart.bwa.x.in','r') as ioText:
         startVar = int(ioText.readlines()[0].strip())+1
     print("Start value is ",str(startVar))
     time.sleep(5)

file_array = []
with open('filelist.txt', 'r') as iowrapper:
    csvreader = csv.reader(iowrapper, delimiter=',')
    for row in csvreader:
        file_array.append(row[0])

for i in range(startVar,70):
    filebase = file_array[i]
    os.system(f"bwa mem -t 20 -T 40 -L 500 -B 16 -M mylu.rm.bwa {filebase}.R1.uni.repair.filter.g.kmer.fastq.gz {filebase}.R2.uni.repair.filter.g.kmer.fastq.gz > $
    print(i)
    # write i into restart file
    with open('restart.bwa.x.in','w') as iowrapper:
        iowrapper.write(str(i))


##convert sam files to bam files with samtools
for f in `ls -1 *.sam | sed 's/.sam//' `
do
samtools view -@ 20 -bS ${f}.sam > ${f}.bam
done


##sort bam files with samtools
for f in `ls -1 *.bam | sed 's/.bam//' `
do
samtools sort ${f}.bam -@ 20 -o ${f}.sorted.bam
done


##remove improperly paired reads with samtools
for f in `ls -1 *.sorted.bam | sed 's/.sorted.bam//' `
do
samtools view -b -f 0x2 ${f}.sorted.bam -@ 20 > ${f}.sorted.prop.bam
done


##analyze data with samtools flagstat
for f in `ls -1 *.prop.bam | sed 's/.prop.bam//' `
do
samtools flagstat ${f}.prop.bam -@ 20 > outputfiles/${f}.prop.out.flag
done


##filter duplicates with picard
for f in `ls -1 *.sorted.prop.bam | sed 's/.sorted.prop.bam//' `
do
picard -Xmx100g MarkDuplicates \
      REMOVE_DUPLICATES=true \
      I=${f}.sorted.prop.bam \
      O=${f}.sorted.prop.dups.bam \
      M=${f}.marked_dup_metrics.txt
done


##calculate coverage metrics with picard
for f in `ls -1 *.dups.bam | sed 's/.dups.bam//' `
do
picard -Xmx100g CollectWgsMetrics \
      I=${f}.dups.bam \
      O=${f}.coverage.txt \
      R=/scratch/rek89/genome/mylu.genome.rm.gmap.k100.fa
done


##index with samtools
for f in `ls -1 *.sorted.prop.dups.bam | sed 's/.sorted.prop.dups.bam//' `
do
samtools index ${f}.sorted.prop.dups.bam -@ 20
done


#######################################################
########variant call/post-variant filtering############
#######################################################

##call variants for all samples with ANGSD. skno.new.filelist is a list of all bam files to be included in call.

angsd -bam skno.new.filelist -nThreads 20 -doGLF 2 -GL 1 -doMajorMinor 1 -doMaf 1 -doCounts 1 -doPost 1 -minMapQ 30 -minQ 20 -SNP_pval 1e-6 -minMaf 0.05 -minInd 123 -setMaxDepth 2460 -out skno.new



#####filter for allele balance based on Pinsky et al., 2021
#keep only first 4 columns
cut -f1-4 skno.new.mafs > skno.new.mafs.sites.txt

#remove header
tail -n+2 skno.new.mafs.sites.txt > skno.new.mafs.sites1.txt

#create bed file
awk '{OFS="\t"; print $1, $2-1, $2}' skno.new.sites1.sorted.txt > skno.new.sites.bed

# Set paths
REFERENCE="mylu.genome.rm.gmap.k100.fa"
POSITIONS="/sodalis/SNP4/skno.new.sites.bed"

# Create output directory
mkdir -p out.bamreadcount

# Export variables so they are available in parallel subshells
export REFERENCE
export POSITIONS


# Define the function and get read counts per locus with bam-readcount
run_readcount() {
  BAM="$1"
  SAMPLE=$(basename "$BAM" .dups.bam)
  echo "Processing $SAMPLE..."
  bam-readcount -f "$REFERENCE" -l "$POSITIONS" "$BAM" > "out.bamreadcount/${SAMPLE}.readcount.txt"
}

export -f run_readcount

# Run in parallel across all .dups.bam files, 20 at a time
parallel --compress -j 20 run_readcount ::: *.dups.bam

#concatenate files; chatgpt assist
# Header
echo -e "scaffold\tposition\tallele1_count\tallele2_count" > all.counts.tsv

# Loop through gzipped readcount files
for file in *.readcount.txt.gz; do
  zcat "$file" | awk -v sample="$file" '
  {
    split($5, a1, ":"); split($6, a2, ":");
    a1c = a1[2]; a2c = a2[2];
    total = a1c + a2c;
    # Define heterozygote: both alleles present with >5 reads each (adjust as needed)
    if (a1c > 5 && a2c > 5) {
      key = $1"\t"$2;
      hets[key]["a1"] += a1c;
      hets[key]["a2"] += a2c;
    }
  }
  END {
    for (k in hets) {
      split(k, loc, "\t");
      print loc[1]"\t"loc[2]"\t"hets[k]["a1"]"\t"hets[k]["a2"];
    }
  }'
done >> all.counts.tsv

##binomial test for allelic imbalance and generation of filter table
Rscript --no-save binom.test

# Load data
df <- read.table("all.counts.tsv", header = TRUE, sep = "\t")

# Total reads at each locus
df$total_count <- df$allele1_count + df$allele2_count

# Run binomial test for each locus (H0: p = 0.5 expected balance)
# If reference/alternate allele bias exists, this test will catch it
binom_results <- apply(df, 1, function(row) {
  binom.test(x = as.numeric(row["allele1_count"]),
             n = as.numeric(row["total_count"]),
             p = 0.5)$p.value
})

# Add p-values to dataframe
df$p_value <- binom_results

# FDR correction
df$fdr <- p.adjust(df$p_value, method = "fdr")

# Filter for loci with FDR < 0.05
significant <- df[df$fdr < 0.05, c("scaffold", "position", "allele1_count", "allele2_count", "p_value", "fdr")]

# Save results
write.table(significant, file = "imbalanced.loci.tsv", sep = "\t", quote = FALSE, row.names = FALSE)
df <- read.table("imbalanced.loci.tsv", header = TRUE)
write.table(df[, c("scaffold", "position")],
            file = "imbalanced.sites.txt",
            sep = "\t", quote = FALSE, row.names = FALSE, col.names = FALSE)

sort imbalanced.sites.txt > imbalanced.sites.sort.txt

##filter mafs and beagle file for loci with systemic allele balance issues
zcat skno.new.mafs.gz | awk 'NR==FNR {bad[$1":"$2]; next} !($1":"$2 in bad)' imbalanced.sites.sort.txt - | gzip > skno.new.imb.mafs.gz


zcat skno.new.beagle.gz | awk 'FNR==NR {bad[$1"_"$2]; next}
    NR==1 || !( $1 in bad )' imbalanced.sites.sort.txt - | gzip > skno.new.imb.beagle.gz

###Missingness. Count proportion of "0.3333333" calls in a beagle file. chatgpt assist
# Path to BEAGLE file (plain text)
BEAGLE_FILE="skno.new.imb.beagle"  # <-- Replace with your actual file name

# Output file
OUTPUT_FILE="skno.new.missingness_summary.imb.tsv"

awk -v out="$OUTPUT_FILE" '
BEGIN {
  FS=OFS="\t"
}
NR==1 {
  # Header: extract sample names (every 3rd column starting from column 4)
  for (i=4; i<=NF; i+=3) {
    sample[++s] = $i
    miss[$i] = 0
  }
  next
}
{
  total++
  idx = 0
  for (i=4; i<=NF; i+=3) {
    idx++
    a = $(i) + 0
    b = $(i+1) + 0
    c = $(i+2) + 0
    # Count as missing if all three values are exactly 0.333333
    if (a == 0.333333 && b == 0.333333 && c == 0.333333) {
      miss[sample[idx]]++
    }
  }
}
END {
  print "Sample", "MissingSites", "TotalSites", "MissingRate" > out
  for (i=1; i<=s; i++) {
    name = sample[i]
    rate = miss[name] / total
    print name, miss[name], total, rate > out
  }
  print "Wrote output to " out > "/dev/stderr"
}' "$BEAGLE_FILE"
